Kaiming Init on all linear 
10.788929 M parameters
step 0: train loss 5.1537, val loss 5.1484
step 500: train loss 2.1614, val loss 2.2129
step 1000: train loss 1.8199, val loss 1.9622
step 1500: train loss 1.6347, val loss 1.8113
step 2000: train loss 1.5267, val loss 1.7293
step 2500: train loss 1.4429, val loss 1.6606
step 3000: train loss 1.3853, val loss 1.6210
step 3500: train loss 1.3407, val loss 1.5838
step 4000: train loss 1.3010, val loss 1.5560
step 4500: train loss 1.2661, val loss 1.5401
step 4999: train loss 1.2355, val loss 1.5260

Using normal distribution 
10.788929 M parameters
step 0: train loss 4.1934, val loss 4.1999
step 500: train loss 1.7371, val loss 1.8896
step 1000: train loss 1.3926, val loss 1.6122
step 1500: train loss 1.2695, val loss 1.5249
step 2000: train loss 1.1846, val loss 1.5066
step 2500: train loss 1.1234, val loss 1.4914
step 3000: train loss 1.0664, val loss 1.4799
step 3500: train loss 1.0140, val loss 1.5021
step 4000: train loss 0.9566, val loss 1.5259
step 4500: train loss 0.9022, val loss 1.5486
step 4999: train loss 0.8476, val loss 1.5743

MultiHeadAttention no dropouts in head layer
10.795841 M parameters
step 0: train loss 4.3333, val loss 4.3214
step 500: train loss 1.6547, val loss 1.8132
step 1000: train loss 1.2865, val loss 1.5561
step 1500: train loss 1.1239, val loss 1.5385
step 2000: train loss 0.9721, val loss 1.6022
step 2500: train loss 0.8054, val loss 1.7511
step 3000: train loss 0.6254, val loss 1.9475
step 3500: train loss 0.4722, val loss 2.1539
step 4000: train loss 0.3520, val loss 2.4342
step 4500: train loss 0.2702, val loss 2.6705
step 4999: train loss 0.2154, val loss 2.9267

MultiHeadAttention encapsulated with dropout
10.795841 M parameters
step 0: train loss 4.2663, val loss 4.2604
step 500: train loss 1.7550, val loss 1.9038
step 1000: train loss 1.4021, val loss 1.6248
step 1500: train loss 1.2823, val loss 1.5405
step 2000: train loss 1.1902, val loss 1.4987
step 2500: train loss 1.1302, val loss 1.4933
step 3000: train loss 1.0756, val loss 1.4993
step 3500: train loss 1.0202, val loss 1.4941
step 4000: train loss 0.9689, val loss 1.5266
step 4500: train loss 0.9136, val loss 1.5390
step 4999: train loss 0.8600, val loss 1.5719

CNN 
1.261445 M parameters
step 0: train loss 4.1744, val loss 4.1744
step 500: train loss 2.4714, val loss 2.4970
step 1000: train loss 2.4651, val loss 2.4832
step 1500: train loss 2.4639, val loss 2.4891
step 2000: train loss 2.4624, val loss 2.4858
step 2500: train loss 2.4602, val loss 2.4900
step 3000: train loss 2.4597, val loss 2.4889
step 3500: train loss 2.4611, val loss 2.4910
step 4000: train loss 2.4593, val loss 2.4891
step 4500: train loss 2.4621, val loss 2.4880
step 4999: train loss 2.4612, val loss 2.4855